[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Helen Schmidt",
    "section": "",
    "text": "I am a Social Psychology & Quantitative Methods PhD student who studies emotion and language in social contexts using behavioral, computational, and neuroimaging methods.  I work with Dr. Chelsea Helion in the Social and Affective Neuroscience Lab at Temple University in Philadelphia.   My CV is available here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Helen Schmidt and I am a Social Psychology & Quantitative Methods PhD student in the Social and Affective Neuroscience Lab at Temple University. Under the guidance of Dr. Chelsea Helion, my research focuses on the relationship between emotion and language in social contexts. I am passionate about employing a multimodal approach to my research, combining traditional behavioral and neuroimaging tools with machine learning and advanced quantitative methods.\nPreviously, I worked as the lab coordinator in Dr. Maureen Ritchey’s Memory Modulation Lab at Boston College, where I studied how retrieval strategies impacted memory for emotional information. Before that, I completed my undergraduate studies at Tufts University, where I graduated with a BS in biology in 2017. During my time at Tufts, I worked with Dr. Elizabeth Race in the Integrative Cognitive Neuroscience Lab to investigate how attention and mind-wandering impacted memory quality. I also completed a MSc in clinical neuroscience at University College London, where I studied learning and decision-making in affective contexts.\nOutside of the lab, I can be found reading books, exploring new places, drinking coffee, and spending time outside. I’m also a huge ggplot enthusiast - you can find some of my R projects in my portfolio."
  },
  {
    "objectID": "footer.html",
    "href": "footer.html",
    "title": "Helen Schmidt",
    "section": "",
    "text": "© 2022 Helen Schmidt"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "+ denotes equal contribution       # denotes trainee \nUlichney, V., Schmidt, H., & Helion, C. (under review). Maybe the real prize is the friends we made along the way: Perceived support from friends enhances everyday emotional well-being in the U.S. and Japan. Under review at Personality and Social Psychology Bulletin, June 14, 2023.  \n\nEckardt, D., Helion, C., Schmidt, H., Chen, J., & Murty, V.P. (under review). Storytelling changes the structure and perceived value of event memories. Under review at Cognition, May 31, 2023.\n\nSchmidt, H.+, Tran, S.+#, Medaglia, J., Ulichney, V., & Helion, C. (under review). Conversational linguistic features predict social network learning. Under review at Psychonomic Bulletin & Review, May 15, 2023.  \n\nMitchell, W.J., Stasiak, J., Martinez, S.A., Cliver, K., Gregory, D.F., Reisman, S., Schmidt, H., Murty, V.P., & Helion, C. (preprint). Emotion regulation strategy usage in a dynamic, high-intensity context."
  },
  {
    "objectID": "publications.html#manuscripts-in-preparation",
    "href": "publications.html#manuscripts-in-preparation",
    "title": "Publications",
    "section": "Manuscripts In Preparation",
    "text": "Manuscripts In Preparation\nSchmidt, H. & Helion, C. (in prep.). Quantitative methods in affective science: Recent advances, theoretical implications, and future directions.\n\nSchmidt, H., Mitchell, W.J., & Helion, C. (in prep.). Emotion regulatory effects of emotion and content labeling in a naturalistic context."
  },
  {
    "objectID": "publications.html#selected-poster-presentations",
    "href": "publications.html#selected-poster-presentations",
    "title": "Publications",
    "section": "Selected Poster Presentations",
    "text": "Selected Poster Presentations\nClick here to see a selection of my poster presentations. All presentations are listed on my CV."
  },
  {
    "objectID": "selected-posters.html",
    "href": "selected-posters.html",
    "title": "Selected Poster Presentations",
    "section": "",
    "text": "Poster PDF       # denotes trainee"
  },
  {
    "objectID": "selected-posters.html#section",
    "href": "selected-posters.html#section",
    "title": "Selected Poster Presentations",
    "section": "2023",
    "text": "2023\nCunningham, R.M., Quarmley, M., Calderaro, M., Clarkson, T., Schmidt, H., Cassidy, C.M., & Jarcho, J.M. (April). Relations between neuromelanin in substantia nigra and peer-based aggression in adolescents are potentiated by irritability. Poster presented at the Social and Affective Neuroscience Society Annual Meeting, Santa Barbara, CA.\nOsuntokun, M.#, Schmidt, H., & Helion, C. (April). Can linguistic aspects predict relationship formation and future outcomes? Poster presented at the Temple University Psychology Honors Poster Session, Philadelphia, PA.\nMartinez, S.A., Cliver, K., Mitchell, W.J., Schmidt, H., Helion, C., Chein, J., & Murty, V.P. (March). Linguistic cues of memory accuracy differ for low-threat and high-threat memories. Poster presented at the Cognitive Neuroscience Society Annual Meeting, San Francisco, CA.\nSchmidt, H. & Helion, C. (March). Emotion regulatory effects of emotion and content labeling in a naturalistic context. Poster presented at the Society for Affective Science Annual Conference, Long Beach, CA. \nSchmidt, H., Tran, S.#, & Helion, C. (February). Conversations predict social network learning. Poster presented at the Society for Personality and Social Psychology Annual Convention, Atlanta, GA."
  },
  {
    "objectID": "selected-posters.html#section-1",
    "href": "selected-posters.html#section-1",
    "title": "Selected Poster Presentations",
    "section": "2022",
    "text": "2022\nEckardt, D., Helion, C., Schmidt, H., Chen, J., & Murty, V.P. (November). What makes a story: Uncovering mechanisms of memory communication. Poster presented at the Psychonomic Society Annual Meeting, Boston, MA.\nSchmidt, H., Tran, S.#, & Helion, C. (May). Conversational linguistic features that inform social network learning. Poster presented at the Philadelphia Decision Neuroscience Symposium, Philadelphia, PA."
  },
  {
    "objectID": "selected-posters.html#section-2",
    "href": "selected-posters.html#section-2",
    "title": "Selected Poster Presentations",
    "section": "2020",
    "text": "2020\nSchmidt, H. & Ritchey, M. (August). The protective effects of retrieval practice on positive memories. Poster presented virtually at the Context and Episodic Memory Symposium. \nSchmidt, H., Cooper, R.A., & Ritchey, M. (May). Temporal dynamics supporting the multidimensional quality of episodic memory. Poster presented virtually at the Cognitive Neuroscience Society Annual Meeting."
  },
  {
    "objectID": "selected-posters.html#section-3",
    "href": "selected-posters.html#section-3",
    "title": "Selected Poster Presentations",
    "section": "2019",
    "text": "2019\nSchmidt, H., Samide, R., Cooper, R.A., & Ritchey, M. (May). News flash! Investigating the dynamics of emotional memory using real-life event videos. Poster presented at the Context and Episodic Memory Symposium, Philadelphia, PA."
  },
  {
    "objectID": "selected-posters.html#section-4",
    "href": "selected-posters.html#section-4",
    "title": "Selected Poster Presentations",
    "section": "2018",
    "text": "2018\nDistefano, D., Schmidt, H., Hickey, P., & Race, E. (March). Characterizing EEG signatures of inattention that predict forgetting. Poster presented at the Cognitive Neuroscience Society Annual Meeting, Boston, MA."
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Introduction to Machine Learning in R\n\n\n\n\n\n\n\nR\n\n\nOpen Data\n\n\nMachine Learning\n\n\n\n\nA tutorial created for the 2023 Temple University Coding Outreach Group summer workshop series.\n\n\n\n\n\n\nJun 15, 2023\n\n\nHelen Schmidt\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Chart Challenge\n\n\n\n\n\n\n\nData Viz\n\n\nR\n\n\nOpen Data\n\n\n\n\nIn April 2023, I took part in the #30DayChartChallenge, a month-long challenge to create creative data visualizations around a selection of topics.\n\n\n\n\n\n\nMay 1, 2023\n\n\nHelen Schmidt\n\n\n\n\n\n\n  \n\n\n\n\n30 Day Map Challenge\n\n\n\n\n\n\n\nMaps\n\n\nR\n\n\nOpen Data\n\n\n\n\nThroughout November 2022, I took part in the #30DayMapChallenge, a month-long challenge to create original maps using geospatial data. Check out the results!\n\n\n\n\n\n\nApr 1, 2023\n\n\nHelen Schmidt\n\n\n\n\n\n\n  \n\n\n\n\nData Visualizations in R\n\n\n\n\n\n\n\nData Viz\n\n\nR\n\n\nOpen Data\n\n\n\n\nUsing open-source data and tidyverse packages, this demo explores different data wrangling and plotting techniques in R.\n\n\n\n\n\n\nMar 7, 2023\n\n\nHelen Schmidt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-04-01-Map-Challenge/index.html",
    "href": "posts/2023-04-01-Map-Challenge/index.html",
    "title": "30 Day Map Challenge",
    "section": "",
    "text": "Every November, the #30DayMapChallenge encourages data visualization enthusiasts to create original maps using publicly-available geospatial data. Every day presented a new theme, and I used R to create my maps around these themes. This was a fun experience, and I was really proud of how much I learned about working with geospatial data in R by the end of the month. All corresponding code and individual maps can be found on my GitHub."
  },
  {
    "objectID": "posts/2023-03-07-Data-Viz/index.html",
    "href": "posts/2023-03-07-Data-Viz/index.html",
    "title": "Data Visualizations in R",
    "section": "",
    "text": "This demo uses an open-source dataset to explore different data wrangling and plotting techniques in R. Data organization is hugely important because it can impact the quality and accuracy of any statistical tests or visualizations. A popular collection of data organization packages is the tidyverse, which all share an “underlying design philosophy, grammar, and data structure” (see more).\nThe basic structure of tidy data is every variable goes into a column and every column is a variable. Using this framework, we can manipulate the data to calculate new values, run statistical tests, and generate a graphic. In this demo, I will primarily use dplyr, ggplot2, and tidyr to organize my data and make some beautiful plots. I will be using the Top Hits Spotify from 2000 - 2019 dataset, available on Kaggle."
  },
  {
    "objectID": "posts/2023-03-07-Data-Viz/index.html#load-packages-and-data",
    "href": "posts/2023-03-07-Data-Viz/index.html#load-packages-and-data",
    "title": "Data Visualizations in R",
    "section": "Load packages and data",
    "text": "Load packages and data\nFirst, I will load the packages I need. If you do not already have tidyverse packages installed on your computer, you should install them using install.packages('tidyverse') first. The other packages I’m loading will be useful for customizing my plots. I’ll also set a global theme to theme_classic.\n\nlibrary(ggpubr)                 # ggplot customization\nlibrary(Rmisc)                  # basic statistics helper\nlibrary(reactable)              # format tables in a nice way\nlibrary(gganimate)              # animate plots\nlibrary(scales)                 # ggplot scale customization\nlibrary(icons)                  # icon library\nlibrary(tidyverse)              # load all tidyverse packages\ntheme_set(theme_classic())      # set classic theme\n\nI will also read in the dataset as df_raw and look at the first few rows to get a sense of the variables I’m working with.\n\n# read in data\ndf_raw <- read.csv('./data/songs_normalize.csv')\n# see first six rows of all variables\nreactable(head(df_raw), compact = T, resizable = T)\n\n\n\n\n\n\nBased on the data, it looks like I have 18 variables. These are further explained on the Kaggle page for this dataset. This is a lot of data, so it’s useful to break down and organize the data depending on my analysis questions. This is where dplyr and tidyr come in handy.\nI also noticed that despite the dataset saying it includes songs from 2000 to 2019, I see some songs from before 2000 and after 2019 included. I will remove those observations.\n\ndf <- subset(df_raw, df_raw$year >= 2000 & df_raw$year <= 2019)"
  },
  {
    "objectID": "posts/2023-03-07-Data-Viz/index.html#analysis-plan",
    "href": "posts/2023-03-07-Data-Viz/index.html#analysis-plan",
    "title": "Data Visualizations in R",
    "section": "Analysis plan",
    "text": "Analysis plan\nSince I have so much data, I’ll want to narrow down my analysis questions for this demo. The main questions I will explore are:\n\n\n   Which artists have the most hit songs?\n\n\n   Are positive songs more energetic and danceable than negative songs?\n\n\n   Do songs in major and minor scale change in popularity over time?\n\n\n   Are songs with explicit lyrics speechier than songs without explicit lyrics?\n\n\n   Does song tempo or duration influence song popularity?\n\nEach of these questions will highlight a different data visualization method. In my experience, it can be helpful to test different plotting methods to find the best way to display results.\n\n  – Which artists have the most hit songs?\nIn order to find out which artists have the most hit songs, I need to count the number of songs by every artist in the data frame. I can easily do this using %>% (pipe) notation, which allows me to express a sequence of multiple operations. The pipe comes from the magrittr package, but tidyverse loads it automatically. Pipes allow me to write a step-by-step command that is executed in a certain order.\nHere, I gather the data contained in df and I ultimately want to store it in a new data frame called artists. To do this, I first group all the data by the unique artist name. I know that this is the first step because it’s the first statement that comes after my initial %>%. Then, while grouping the data by artist, I can count the number of songs by grabbing the length of the song variable.\nFinally, I want to see a list of the top ten artists in descending order by the number of songs.\n\nartists <- df %>%                      # create new data frame\n  group_by(artist) %>%                 # group by unique artist name\n  summarize(SongCount = length(song))  # count the number of songs\n\n# sort list in descending order of number of songs\nartists <- arrange(artists, desc(SongCount))\n\n# print table of top 10 artists\nreactable(head(artists, n = 10), compact = T)\n\n\n\n\n\n\n\nTo visualize this information in a plot, I can save this information as a data frame and make a very simple plot using ggplot2.\n\n# save top ten\nTopTen <- head(artists, n = 10)\n\nggplot(TopTen, aes(x = artist, y = SongCount)) +\n  # outline bars in black, fill with light teal blue\n  geom_col(color = \"black\", fill = \"#fcbc66\", alpha = 0.8) +\n  # order bars in descending order and wrap text so last name appears on second line\n  scale_x_discrete(limits = TopTen$artist, labels = function(x) str_wrap(x, width = 10)) +\n  # label x axis\n  xlab(NULL) + \n  # label y axis\n  ylab(\"Number of Hit Songs\") + \n  # write a descriptive title\n  ggtitle(\"Top Ten Artists with Hit Songs on Spotify from 2000 - 2019\") +\n  # add song count value above each bar\n  geom_text(aes(label = SongCount), position = position_dodge(width = 0.9), vjust = -0.5) \n\n\n\n\n\n\n\n\n\n\n  – Are positive songs more energetic and “danceable” than negative songs?\nIn order to determine if positive songs are both more energetic and more “danceable” than negative songs, I first want to binarize valence into two categories – positive and negative. The Kaggle dataset mentions that songs with a valence greater than 0.5 are considered more positive, while songs with a valence that is less than 0.5 are considered more negative. With this in mind, I will create a new variable called valence.category that reflects this binary split.\nI also want to get some statistical measures for my plot. Using summarySE from the Rmisc package, I can calculate mean, standard deviation, standard error, and 95% confidence intervals for a measurement variable while grouping by another variable. In this case, I want to calculate these statistical measures for both danceability and energy while grouping by the newly created valence.category.\n\n# bin valence values into positive and negative categories\ndf$valence.category[df$valence >= 0.5] <- \"Positive\"\ndf$valence.category[df$valence < 0.5] <- \"Negative\"\n\n# get stats for danceability and energy (mean, 95% confidence interval, etc.)\ndance <- summarySE(df, measurevar = \"danceability\", groupvars = \"valence.category\")\nenergy <- summarySE(df, measurevar = \"energy\", groupvars = \"valence.category\")\n\nNow I can create my plot! I will be making a violin plot to show not only the mean difference between my valence groups, but also what the distribution is within my two valence categories. I will create a plot for energy and for danceability, and combine those into one joint plot using ggarrange.\n\n# build violin plot for danceability\ndance.plot <- ggplot(data = df, aes(x = valence.category, y = danceability,\n                                    fill = valence.category, color = valence.category)) +\n  # violin plot\n  geom_violin(scale = \"area\", alpha = 0.8) +\n  # fill with my selected colors\n  scale_fill_manual(values = c(\"#8dc6bf\",\"#fcbc66\")) +\n  scale_color_manual(values = c(\"#8dc6bf\",\"#fcbc66\")) +\n  # add point for mean of each valence category\n  geom_point(data = dance, aes(x = valence.category, y = danceability), color = \"black\") +\n  # add 95% confidence intervals\n  geom_errorbar(data = dance, aes(ymin = danceability-ci, ymax = danceability+ci),\n                width = 0.25, position = \"dodge\", color = \"black\") +\n  # label x axis\n  xlab(NULL) +\n  # label y axis\n  ylab(\"Danceability\") +\n  # don't include legend\n  theme(legend.position = \"none\")\n\n# build violin plot for energy\nenergy.plot <- ggplot(data = df, aes(x = valence.category, y = energy,\n                                     fill = valence.category, color = valence.category)) +\n  # violin plot\n  geom_violin(scale = \"area\", alpha = 0.8) +\n  # fill with my selected colors\n  scale_fill_manual(values = c(\"#8dc6bf\",\"#fcbc66\")) +\n  scale_color_manual(values = c(\"#8dc6bf\",\"#fcbc66\")) +\n  # add point for mean of each valence category\n  geom_point(data = energy, aes(x = valence.category, y = energy), color = \"black\") +\n  # add 95% confidence intervals\n  geom_errorbar(data = energy, aes(ymin = energy-ci, ymax = energy+ci),\n                width = 0.25, position = \"dodge\", color = \"black\") +\n  # label x axis\n  xlab(NULL) +\n  # label y axis\n  ylab(\"Energy\") +\n  # don't include legend\n  theme(legend.position = \"none\")\n\n# combine dance plot and energy plot using ggarrange\nplot <- ggarrange(dance.plot, energy.plot, ncol = 2)\n# add title and note to plot\nannotate_figure(plot, top = text_grob(\"Danceability and Energy in Positive and Negative Songs\",\n                color = \"black\", face = \"bold\", size = 14),\n                bottom = text_grob(\"Bars indicate 95% confidence intervals around the mean.\",\n                                   color = \"black\", face = \"italic\", size = 8))\n\n\n\n\n\n\n\n\nIt looks like positive songs are both more energetic and more danceable than negative songs, which makes sense. Interestingly, negative songs extend to both the lower and higher ends of the energy and danceability scales, while positive songs tend to be more densely clustered toward the higher end of the scales.\n\n\n  – Do songs in major and minor scales change in popularity over time?\nHere I’m interested to see if songs that are in major versus minor scale change in popularity over time. The major scale is a more commonly used scale, especially in Western music. On the flip, side the minor scale is used when musicians want to evoke a feeling of eeriness or suspense (some examples include “Stairway to Heaven” by Led Zeppelin and “Scarborough Fair” by Simon & Garfunkel).\nI first need to organize the data and calculate an average popularity score for each scale type (the mode variable) and for each year. Then I will create a new character variable called mode.char that indicates whether the scale is major or minor.\n\npopularity <- df %>%\n  group_by(year,mode) %>%\n  summarize(MeanPopularity = mean(popularity))\n\npopularity$mode.char[popularity$mode == 1] <- \"Major\"\npopularity$mode.char[popularity$mode == 0] <- \"Minor\"\n\nNow I can make my plot! Because I’m showing change over time, I decided to use the gganimate package to reveal each data point sequentially on an animated plot.\n\n# make plot\nplot <- ggplot(popularity, aes(x = year, y = MeanPopularity, group = mode.char)) +\n  # animate to reveal points over time\n  transition_reveal(year) +\n  # add point for each year\n  geom_point(aes(color = mode.char), size = 2) +\n  # connect points with line\n  geom_line(aes(color = mode.char), size = 1.1) +\n  # show all years on the x-axis\n  scale_x_continuous(breaks = pretty_breaks(n=20)) +\n  # label x-axis and y-axis\n  ylab(\"Average Popularity\") + xlab(\"Year\") +\n  # add a descriptive title\n  ggtitle(\"Average Popularity of Major and Minor Songs from 2000 - 2019\") +\n  # use my colors and change legend title\n  scale_color_manual(values = c(\"#8dc6bf\",\"#fcbc66\"), name = \"Scale\") +\n  # angle and move x-axis labels and text\n  theme(\n    axis.text.x = element_text(hjust = 1, angle = 45),\n    axis.title.x = element_text(vjust = -1)\n    )\n\n# animate\nanimate(plot, duration = 8, fps = 20, renderer = gifski_renderer(), end_pause = 60)\n\n\n\n\n\n\n\n\n\n\n  – Are songs with explicit lyrics “speechier” than songs without explicit lyrics?\nIn this dataset, “speechiness” is a measure of spoken words in a song. Songs with more exclusively speech-like contents (like a talk show, podcast, etc.) have scores between 0.66 and 1. Songs with a speechiness value between 0.33 and 0.66 describe tracks that contain both music and speech. This range is where I’d expect most of these songs within. Finally, songs with values below 0.33 represent instrumental songs and songs with more music than words.\nFirst, I want to see the range of speechiness values in my dataset to see where songs in this dataset fall.\n\n# find range of speechiness variable\nmin <- min(df$speechiness)\nmax <- max(df$speechiness)\n\n# show histogram of speechiness scores\nhist(df$speechiness, col = \"#FF6347\", xlab = \"Speechiness\", main = \"Histogram of Speechiness Values\")\n\n\n\n# print range\npaste0(\"Speechiness values range from \", min, \" to \", max, \" in this dataset.\", sep = \"\")\n\n[1] \"Speechiness values range from 0.0232 to 0.576 in this dataset.\"\n\n\nInteresting! In this dataset, there are actually more songs that have more instrumental music. I wonder if songs that contain more speech also contain more explicit dialogue than songs with less speech. I can investigate this question using a density plot.\n\ndf$explicit.char[df$explicit == \"False\"] <- \"No\"\ndf$explicit.char[df$explicit == \"True\"] <- \"Yes\"\n\nggplot(df, aes(speechiness)) +\n  # create density plot to show distribution\n  geom_density(aes(fill = factor(explicit.char)), alpha = 0.8) +\n  # use my colors and rename legend\n  scale_fill_manual(values = c(\"#8dc6bf\",\"#fcbc66\"), name = \"Explicit Lyrics\") +\n  # label x-axis and y-axis\n  ylab(\"Density\") + xlab(\"Average Track Speechiness\") +\n  # add a descriptive title\n  ggtitle(\"Average Speechiness of Songs With and Without Explicit Lyrics\")\n\n\n\n\n\n\n\n\nFrom the density plot, we see that yes, songs with more speech and music tend to have more explicit language than songs with less speech!\n\n\n  – Does song tempo or duration influence song popularity?\nFinally, I want to investigate if there is a relationship between song tempo and song popularity or between song duration and song popularity. To do this, I’m going to run two simple linear regressions using base R’s lm function. I want to see if song tempo (independent, predictor variable) influences song popularity (dependent, response variable). I also want to see if song duration in minutes influences song popularity.\n\n# does song tempo influence popularity?\nggplot(df, aes(x = tempo, y = popularity)) +\n  # show data points\n  geom_jitter(size = 1, shape = 1) +\n  # draw regression line\n  geom_smooth(method = \"lm\", color = \"#8dc6bf\", fill = \"#8dc6bf\") +\n  # label x-axis and y-axis\n  xlab(\"Tempo (Beats Per Minute)\") + ylab(\"Average Popularity\")\n\n\n\n\n\n\n\n# run a linear regression\nmodel1 <- lm(popularity ~ tempo, data = df)\nsummary(model1)\n\n\nCall:\nlm(formula = popularity ~ tempo, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-60.668  -3.518   5.860  13.439  29.151 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 58.47742    2.21991  26.342   <2e-16 ***\ntempo        0.01106    0.01804   0.613     0.54    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.51 on 1956 degrees of freedom\nMultiple R-squared:  0.0001921, Adjusted R-squared:  -0.000319 \nF-statistic: 0.3759 on 1 and 1956 DF,  p-value: 0.5399\n\n\nBased on the regression results, there does not appear to be an effect of song tempo on popularity (p = .54).\n\n# calculate song duration in minutes\ndf$duration_min <- df$duration_ms/60000\n\n# does song duration influence popularity?\nggplot(df, aes(x = duration_min, y = popularity)) +\n  # show data points\n  geom_jitter(size = 1, shape = 1) +\n  # draw regression line\n  geom_smooth(method = \"lm\", color = \"#fcbc66\", fill = \"#fcbc66\") +\n  # label x-axis and y-axis\n  xlab(\"Song Duration (Minutes)\") + ylab(\"Average Popularity\")\n\n\n\n\n\n\n\n# run a linear regression\nmodel2 <- lm(popularity ~ duration_min, data = df)\nsummary(model2)\n\n\nCall:\nlm(formula = popularity ~ duration_min, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.951  -3.755   5.791  13.531  28.854 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   53.3904     2.8842  18.511   <2e-16 ***\nduration_min   1.6860     0.7472   2.256   0.0242 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.49 on 1956 degrees of freedom\nMultiple R-squared:  0.002596,  Adjusted R-squared:  0.002086 \nF-statistic: 5.091 on 1 and 1956 DF,  p-value: 0.02416\n\n\nBased on the regression results, there is a significant effect of song duration on popularity (p = .024). As songs get longer, they increase in popularity."
  },
  {
    "objectID": "posts/2023-05-01-Chart-Challenge/index.html",
    "href": "posts/2023-05-01-Chart-Challenge/index.html",
    "title": "30 Day Chart Challenge",
    "section": "",
    "text": "In April 2023, I took part in the #30DayChartChallenge, a community-driven event with the goal of creating unique and creative data visualizations. Every day presented a new topic, and I used R to create my charts around these topics. While I didn’t complete as many days of the challenge as I would have liked, I’m happy to have had the excuse to try some new ggplot2 visualizations. All corresponding code and individual charts can be found on my GitHub."
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html",
    "href": "posts/2023-06-15-Machine-Learning/index.html",
    "title": "Introduction to Machine Learning in R",
    "section": "",
    "text": "# define packages we need\npackages <- c(\"tidyverse\", \"palmerpenguins\", \"stats\", \"factoextra\", \"cluster\", \"formatR\", \"class\", \"modelsummary\", \"kableExtra\", \"vembedr\", \"gridExtra\", \"corrr\", \"ggcorrplot\", \"FactoMineR\", \"rpart\", \"rpart.plot\")\n\n# install packages  \n#install.packages(packages)\n\n# load packages\nlapply(packages, require, character.only = TRUE, quietly = TRUE)\n\nMachine learning (ML) broadly describes the process of teaching computers to learn information. It is widely used in computer science, neuroscience, and data science to train “machines” (e.g., algorithms, models) to learn about information and make predictions or classifications based on that learning. Machine learning is a subset of artificial intelligence (AI), and it has grown in popularity during the recent AI boom. While AI describes the general development of computers that can mimic cognitive function, machine learning is more specific, using algorithms and large amounts of data to make informed decisions for specific tasks.\nMachine learning has become especially popular for researchers in psychology and neuroscience. Thanks to huge increases in computing power and access to datasets with thousands or millions of observations, psychologists have seen great success in applying ML methods to gather insights about human behavior and cognition. ML models can reduce time, effort, and error costs and can be applied flexibly across projects and datasets, including behavioral, survey, and neuroimaging data.\nThe goal of this tutorial is to introduce a variety of machine learning methods that can be applied to psychological data. We will examine the differences between unsupervised and supervised models and in which contexts they are best applied. This tutorial will also touch on model evaluation and comparison, and unpack how to determine certain parameters given your data. This is by no means an exhaustive list of all approaches - these are meant to serve as introductory examples and methods that can be employed specifically in the R programming language. A majority of more complex ML approaches are supported in Python using libraries like PyTorch, TensorFlow, and Keras, and these are more typically what you might see in other ML tutorials.\nThis tutorial is intended to be an introduction to machine learning with R, but you should have some familiarity with scripting and R before starting this tutorial. This tutorial will also discuss reasons for using machine learning alongside more “traditional” data analysis and visualization techniques, and leave room for future directions on applying machine learning tools to relevant psychological and neural datasets.\n\n\n\n\n\n\n\n\n\nWe will use one publicly available dataset throughout the tutorial, accessible through the palmerpenguins R package. This is similar to the native iris dataset in R but serves as a fresh and similarly simple dataset.\nNote: Artwork by @allison_horst. All images included in this tutorial are available for open use under the Creative Commons Attribution 4.0 International License.\n\n\n\n\n# set colors\ncolors <- c(\"#ff6800\", \"#d74fd0\", \"#007176\")\n# set custom plot formatting\nplot.format <- list(theme_classic(), scale_color_manual(values = colors), scale_fill_manual(values = colors))"
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html#explore-data",
    "href": "posts/2023-06-15-Machine-Learning/index.html#explore-data",
    "title": "Introduction to Machine Learning in R",
    "section": "Explore data",
    "text": "Explore data\nBefore we start with our ML models, let’s take a moment to explore the data we’re working with.\n\n# load data\ndf <- palmerpenguins::penguins\n# examine data\nhead(df)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex <fct>, year <int>\n\ndatasummary_skim(df, type = \"categorical\")\n\n\n\n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    species \n    Adelie \n    152 \n    44.2 \n  \n  \n     \n    Chinstrap \n    68 \n    19.8 \n  \n  \n     \n    Gentoo \n    124 \n    36.0 \n  \n  \n    island \n    Biscoe \n    168 \n    48.8 \n  \n  \n     \n    Dream \n    124 \n    36.0 \n  \n  \n     \n    Torgersen \n    52 \n    15.1 \n  \n  \n    sex \n    female \n    165 \n    48.0 \n  \n  \n     \n    male \n    168 \n    48.8 \n  \n\n\n\n\ndatasummary_skim(df, type = \"numeric\")\n\n\n\n \n  \n      \n    Unique (#) \n    Missing (%) \n    Mean \n    SD \n    Min \n    Median \n    Max \n       \n  \n \n\n  \n    bill_length_mm \n    165 \n    1 \n    43.9 \n    5.5 \n    32.1 \n    44.5 \n    59.6 \n     \n\n\n  \n  \n    bill_depth_mm \n    81 \n    1 \n    17.2 \n    2.0 \n    13.1 \n    17.3 \n    21.5 \n     \n\n\n  \n  \n    flipper_length_mm \n    56 \n    1 \n    200.9 \n    14.1 \n    172.0 \n    197.0 \n    231.0 \n     \n\n\n  \n  \n    body_mass_g \n    95 \n    1 \n    4201.8 \n    802.0 \n    2700.0 \n    4050.0 \n    6300.0 \n     \n\n\n  \n  \n    year \n    3 \n    0 \n    2008.0 \n    0.8 \n    2007.0 \n    2008.0 \n    2009.0 \n     \n\n\n  \n\n\n\n\n# remove penguins with missing data\ndf <- na.omit(df)\n# correlation tables\ndatasummary_correlation(df[, c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\",\n    \"body_mass_g\")])\n\n\n\n \n  \n      \n    bill_length_mm \n    bill_depth_mm \n    flipper_length_mm \n    body_mass_g \n  \n \n\n  \n    bill_length_mm \n    1 \n    . \n    . \n    . \n  \n  \n    bill_depth_mm \n    −.23 \n    1 \n    . \n    . \n  \n  \n    flipper_length_mm \n    .65 \n    −.58 \n    1 \n    . \n  \n  \n    body_mass_g \n    .59 \n    −.47 \n    .87 \n    1 \n  \n\n\n\n\n\n\n# frequency of body mass\nggplot(data = df, aes(x = body_mass_g, color = species, group = species, fill = species)) +\n    geom_histogram(alpha = 0.7) + labs(x = \"body mass (g)\", y = \"frequency\",\n    title = \"Frequency of Body Masses by Species\") + plot.format\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n# bill length vs. bill depth\nggplot(data = df, aes(x = bill_length_mm, y = bill_depth_mm, color = species,\n    group = species)) + geom_point() + geom_smooth(method = lm, se = F) + labs(x = \"bill length (mm)\",\n    y = \"bill depth (mm)\", title = \"Penguin Bill Length and Bill Depth by Species\") +\n    plot.format\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# body mass vs. flipper length\nggplot(data = df, aes(x = body_mass_g, y = flipper_length_mm, color = species,\n    group = species)) + geom_point() + geom_smooth(method = lm, se = F) + labs(x = \"body mass (g)\",\n    y = \"flipper length (mm)\", title = \"Penguin Body Mass and Flipper Length by Species\") +\n    plot.format\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n# just for fun radar plot\nlibrary(fmsb)\n# create a function to normalize the predictors\nnor <- function(x) {\n    (x - min(x))/(max(x) - min(x))\n}\n# normalize the predictor columns\npenguin.norm <- as.data.frame(lapply(df[, c(\"body_mass_g\", \"flipper_length_mm\",\n    \"bill_length_mm\", \"bill_depth_mm\")], nor))\npenguin.norm <- cbind(df$species, penguin.norm)\nnames(penguin.norm)[1] <- \"species\"\n# get average data for each penguin species\npenguins <- penguin.norm %>%\n    group_by(species) %>%\n    summarize(`body mass` = mean(body_mass_g), `flipper\\nlength` = mean(flipper_length_mm),\n        `bill length` = mean(bill_length_mm), `bill\\ndepth` = mean(bill_depth_mm))\npenguins$species <- as.character(penguins$species)\nnames <- penguins$species\npenguins <- penguins[, -1]\nrownames(penguins) <- names\npenguins <- rbind(rep(1, 1), rep(0, 1), penguins)\n\nradarchart(penguins, pcol = colors, plwd = 3, plty = 1, cglcol = \"grey\")\n\n\n\n\nNow that we’ve examined the data that we’re working with, let’s outline how we can leverage machine learning to analyze this data.\nA common area of investigation in psychology are individual difference measures. Machine learning can be particularly useful to group data into groups, revealing underlying structures or similarities that may not be apparent to human raters. With the palmer penguin data, we can use the four data points for each penguin (bill length, bill depth, flipper length, and body mass) and their species grouping to develop a model that can be applied to new data (supervised). We can see how well a model can classify penguin data into species groups when we remove the species labels (unsupervised)."
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html#k-nearest-neighbors",
    "href": "posts/2023-06-15-Machine-Learning/index.html#k-nearest-neighbors",
    "title": "Introduction to Machine Learning in R",
    "section": "K-Nearest Neighbors",
    "text": "K-Nearest Neighbors\nK-nearest neighbors (k-NN) is a technique used to classify data into specified classes. Let’s start by using the k-NN algorithm to try to classify our penguin species based on two data dimensions, bill length and bill depth.\n\n# set seed so we get the same results every time\nset.seed(123)\n# generate a random number that is 80% of total number of rows in the\n# dataset; this will be our training set\nran <- sample(1:nrow(df), 0.8 * nrow(df))\n# create a function to normalize the predictors\nnor <- function(x) {\n    (x - min(x))/(max(x) - min(x))\n}\n# normalize the predictor columns\ndf.norm <- as.data.frame(lapply(df[, c(\"bill_length_mm\", \"bill_depth_mm\")],\n    nor))\n# check that normalizing worked\nsummary(df.norm)\n\n bill_length_mm   bill_depth_mm   \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2691   1st Qu.:0.2976  \n Median :0.4509   Median :0.5000  \n Mean   :0.4325   Mean   :0.4839  \n 3rd Qu.:0.6000   3rd Qu.:0.6667  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\n# extract training set\ndf.train <- df.norm[ran, ]\n# extract testing set\ndf.test <- df.norm[-ran, ]\n# extract species name to be used as classification argument in knn\ndf.category <- df[ran, 1]\n\n# run knn function; let's start with 3 neighbors and see how accurate that\n# is\npred <- knn(train = df.train, test = df.test, cl = df.category$species, k = 3)\n\n# show proportion correctly classified\naccuracy <- 100 * sum(df.category$species == pred)/nrow(df.category)\nprint(paste0(\"KNN classification accuracy = \", round(accuracy, digits = 2),\n    \"%\", sep = \"\"))\n\n[1] \"KNN classification accuracy = 39.1%\"\n\n# yikes, not great let's try again with different numbers of neighbors\npred <- knn(train = df.train, test = df.test, cl = df.category$species, k = 10)\naccuracy <- 100 * sum(df.category$species == pred)/nrow(df.category)\nprint(paste0(\"KNN classification accuracy = \", round(accuracy, digits = 2),\n    \"%\", sep = \"\"))\n\n[1] \"KNN classification accuracy = 38.35%\""
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html#classification-decision-trees",
    "href": "posts/2023-06-15-Machine-Learning/index.html#classification-decision-trees",
    "title": "Introduction to Machine Learning in R",
    "section": "Classification / Decision Trees",
    "text": "Classification / Decision Trees\nClassification trees (or decision trees) are another supervised learning technique to draw conclusions about a set of data observations. Classification trees are tree models where the target variable (in our case, the species variable) can take on a set of discrete values (e.g., Chinstrap, Gentoo, Adelie). We can train a decision tree model on a selection of our data and then apply it to a set of test data, and see how accurately the decision tree can categorize penguins with their correct species label.\n\nset.seed(123)\n# shuffle rows to ensure we take a random sample for training and testing\n# sets\nshuffle.index <- sample(1:nrow(df))\nshuffle.df <- df[shuffle.index, ]\n\n# we need to create a testing and training set, 80% trains the model data\n# = dataset to train model, size = percent split, train = T creates train\n# set otherwise test\ncreate_train_set <- function(data, size = 0.8, train = TRUE) {\n    n_row = nrow(data)\n    total_row = size * n_row\n    train_sample <- 1:total_row\n    if (train == TRUE) {\n        return(data[train_sample, ])\n    } else {\n        return(data[-train_sample, ])\n    }\n}\n# make sure to use the new shuffled data frame!\ndf.train <- create_train_set(shuffle.df, 0.8, train = T)\ndf.test <- create_train_set(shuffle.df, 0.8, train = F)\n\n# fit decision tree model\nfit <- rpart(species ~ ., data = df.train, method = \"class\")\nrpart.plot(fit, extra = 106)\n\nWarning: extra=106 but the response has 3 levels (only the 2nd level is\ndisplayed)\n\n\n\n\n# now let's predict on the test dataset\npredicted.test <- predict(fit, df.test, type = \"class\")\ntable_predicted <- table(df.test$species, predicted.test)\ntable_predicted\n\n           predicted.test\n            Adelie Chinstrap Gentoo\n  Adelie        26         2      0\n  Chinstrap      1        14      0\n  Gentoo         0         0     24\n\n# it did pretty well! it correctly predicted 26 Adelie penguins but\n# incorrectly classified 2 Chinstrap penguins as Adelie penguins\n\n# measure performance\naccuracy <- sum(diag(table_predicted))/sum(table_predicted)\naccuracy <- accuracy * 100\nprint(paste0(\"Decision tree classification accuracy = \", round(accuracy, digits = 2),\n    \"%\", sep = \"\"))\n\n[1] \"Decision tree classification accuracy = 95.52%\""
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html#k-means-clustering",
    "href": "posts/2023-06-15-Machine-Learning/index.html#k-means-clustering",
    "title": "Introduction to Machine Learning in R",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is an unsupervised technique that assigns each observation to one of k clusters based on the nearest cluster centroid. We have to specify the number of clusters we might expect from the data (we can also do this iteratively to find the optimal number of clusters based on the data) - this is our k.\n\nI find this series of visualizations to be really helpful in understanding the process of assigning observations to these clusters.\n\nIn this example, we’re going to try using a k of 3 as a hypothesized number of clusters for species and a k of 2 as a hypothesized number of clusters for penguin sex. For simplicity, we will be using only flipper length and bill length observations. K-means clustering can handle more than 2 dimensions of data, but to best outline how k-means clustering compares to the actual structure of the data, we will just use these two for now.\n\n# subset flipper length and bill length from unlabeled data frame\nul.subset <- df.ul %>%\n    select(bill_length_mm, flipper_length_mm)\n\n# let's visualize the data with species labels from our main data frame\nggplot(data = df, aes(x = bill_length_mm, y = flipper_length_mm, color = species,\n    group = species)) + geom_point() + labs(x = \"bill length (mm)\", y = \"flipper length (mm)\",\n    title = \"Penguin Bill Length and Flipper Length by Species\") + plot.format\n\n\n\n# run k-means clustering for species (k = 3)\nset.seed(123)\nk3 <- stats::kmeans(ul.subset, 3, nstart = 25)\nstr(k3)\n\nList of 9\n $ cluster     : int [1:333] 2 2 3 2 2 2 3 2 2 3 ...\n $ centers     : num [1:3, 1:2] 47.6 38.5 45.9 217 187.1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"1\" \"2\" \"3\"\n  .. ..$ : chr [1:2] \"bill_length_mm\" \"flipper_length_mm\"\n $ totss       : num 75148\n $ withinss    : num [1:3] 6521 3301 4037\n $ tot.withinss: num 13859\n $ betweenss   : num 61289\n $ size        : int [1:3] 124 115 94\n $ iter        : int 3\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\nk3\n\nK-means clustering with 3 clusters of sizes 124, 115, 94\n\nCluster means:\n  bill_length_mm flipper_length_mm\n1       47.65000          217.0000\n2       38.45304          187.0522\n3       45.94574          196.8404\n\nClustering vector:\n  [1] 2 2 3 2 2 2 3 2 2 3 2 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 3\n [38] 2 3 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 3 3 2 3 2 2 2 3\n [75] 2 3 2 2 2 3 2 2 2 2 3 3 2 2 2 1 2 3 2 3 2 3 2 2 2 2 3 2 2 3 3 3 2 3 2 3 2\n[112] 3 2 2 2 3 2 3 2 3 2 3 2 1 2 3 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 3 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 3 3 3 2 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 2 3 2 3 3 3 3 3 3 3 2\n[297] 3 2 3 3 3 3 1 3 3 1 2 3 3 3 3 3 1 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 1 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1] 6520.630 3301.413 4036.900\n (between_SS / total_SS =  81.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nfviz_cluster(k3, data = ul.subset) + theme_classic()\n\n\n\n# let's visualize the data using sex labels\nggplot(data = df, aes(x = bill_length_mm, y = flipper_length_mm, color = sex,\n    group = sex)) + geom_point() + labs(x = \"bill length (mm)\", y = \"flipper length (mm)\",\n    title = \"Penguin Bill Length and Flipper Length by Sex\") + plot.format\n\n\n\n# run k-means clustering for sex (k = 2)\nset.seed(123)\nk2 <- stats::kmeans(ul.subset, 2, nstart = 25)\nstr(k2)\n\nList of 9\n $ cluster     : int [1:333] 2 2 2 2 2 2 2 2 2 2 ...\n $ centers     : num [1:2, 1:2] 47.8 41.5 216.2 190.9\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"1\" \"2\"\n  .. ..$ : chr [1:2] \"bill_length_mm\" \"flipper_length_mm\"\n $ totss       : num 75148\n $ withinss    : num [1:2] 8012 12938\n $ tot.withinss: num 20950\n $ betweenss   : num 54198\n $ size        : int [1:2] 133 200\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\nk2\n\nK-means clustering with 2 clusters of sizes 133, 200\n\nCluster means:\n  bill_length_mm flipper_length_mm\n1       47.76241          216.1504\n2       41.48600          190.8700\n\nClustering vector:\n  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [38] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1\n[149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2\n[297] 2 2 1 2 2 2 1 2 1 1 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 1 2 2 1 2 2 1 2 2 1 2\n\nWithin cluster sum of squares by cluster:\n[1]  8011.965 12937.821\n (between_SS / total_SS =  72.1 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nfviz_cluster(k2, data = ul.subset) + theme_classic()\n\n\n\n\nHow do we determine the “optimal” number of clusters based on our data? There are a few different methods we can use to find the optimal k to specify. This process can be somewhat subjective, but we can use three different methods to attempt to reach a consensus. Partitioning the data with k-means clustering aims to minimize the intra-cluster variation.\nThe first method is called the Elbow Method and aims to find a k that has the smallest within-cluster sum of squares (WSS). The total WSS measures how compact the clusters are. The Elbow Method examines total WSS as a function of the number of clusters such that adding another cluster doesn’t improve the total WSS.\n\nset.seed(123)\nfviz_nbclust(ul.subset, kmeans, method = \"wss\")\n\n\n\n\nWe can also use a method called the Average Silhouette Method which measures the quality of a clustering. A high average silhouette width indicates good clustering. With this method, the optimal number for k is the one that maximizes the average silhouette over a range of possible values for k.\n\nset.seed(123)\nfviz_nbclust(ul.subset, kmeans, method = \"silhouette\")\n\n\n\n\nFinally, we can use a statistical method called the Gap Statistic Method which compares data against a null hypothesis. The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under a null reference distribution of the data. The optimal cluster number is a value that maximizes the gap statistic and means that the clustering structure is far from the random uniform distribution of observations.\n\nset.seed(123)\ngap_stat <- clusGap(ul.subset, FUN = kmeans, nstart = 25, K.max = 10, B = 50)\nfviz_gap_stat(gap_stat)\n\n\n\n\nOk so from these three methods, we have a mixture of suggestions. The Elbow Method suggests 4 clusters, the Silhouette Method suggests 2 clusters, and the Gap Statistic Method suggests 4 clusters. There is no clear consensus, but we can be relatively confident that using a k between 2 and 4 is likely optimal. Let’s see how these different k values look with our data!\n\n# calculate kmeans for 2, 3, and 4 centers\nk2 <- kmeans(ul.subset, centers = 2, nstart = 25)\nk3 <- kmeans(ul.subset, centers = 3, nstart = 25)\nk4 <- kmeans(ul.subset, centers = 4, nstart = 25)\n# define plots to compare\np1 <- fviz_cluster(k2, geom = \"point\", data = ul.subset) + ggtitle(\"k = 2\") +\n    theme_classic()\np2 <- fviz_cluster(k3, geom = \"point\", data = ul.subset) + ggtitle(\"k = 3\") +\n    theme_classic()\np3 <- fviz_cluster(k4, geom = \"point\", data = ul.subset) + ggtitle(\"k = 4\") +\n    theme_classic()\n# plot together\ngrid.arrange(p1, p2, p3, nrow = 2)"
  },
  {
    "objectID": "posts/2023-06-15-Machine-Learning/index.html#principal-components-analysis",
    "href": "posts/2023-06-15-Machine-Learning/index.html#principal-components-analysis",
    "title": "Introduction to Machine Learning in R",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\nPrincipal Components Analysis (PCA) is a method to reduce dimensionality in a dataset using unsupervised machine learning models. We have four dimensions for each penguin in our dataset (bill length, bill depth, body mass, and flipper length), but as we discussed in k-means clustering, it’s difficult to visualize all four of these dimensions together. Using PCA, we can transform the data into lower-dimensional space and collapse highly correlated variables together. We can then extract important information and visualize the data more easily.\nThe first step we’ll want to take for PCA is data normalization. We can use the same function we created in the supervised learning section. Then we need to compute a covariance matrix from the normalized data (using the corrr package).\n\n# normalize the unlabeled data frame\ndf.norm <- as.data.frame(lapply(df.ul, nor))\n# compute correlation matrix\ndf.corr <- cor(df.norm)\n# visualize correlation matrix\nggcorrplot(df.corr)\n\n\n\n# conduct PCA\ndf.pca <- princomp(df.corr)\nsummary(df.pca)\n\nImportance of components:\n                         Comp.1     Comp.2      Comp.3 Comp.4\nStandard deviation     1.128537 0.22631151 0.067070224      0\nProportion of Variance 0.958087 0.03852893 0.003384022      0\nCumulative Proportion  0.958087 0.99661598 1.000000000      1\n\n# view loadings of first two components only\ndf.pca$loadings[, 1:2]\n\n                      Comp.1     Comp.2\nbill_length_mm     0.3536264  0.9238103\nbill_depth_mm     -0.5572550  0.1601286\nflipper_length_mm  0.5544175 -0.1353666\nbody_mass_g        0.5069877 -0.3203266\n\n# scree plot\nfviz_eig(df.pca, addlabels = T, barfill = \"deepskyblue3\", barcolor = \"deepskyblue3\")\n\n\n\n# biplot of attributes\nfviz_pca_var(df.pca, col.var = \"deepskyblue3\")"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "“How applying to graduate school works” - the Sokol-Hessner Lab \nHarvard Psychology’s PhD Resources and Online Tips \n“Mitch’s Uncensored Advice for Applying to Graduate School in Clinical Psychology” - Mitch Prinstein (useful for both clinical and non-clinical programs) \nTemple University’s Doctoral Program in Psychology & Neuroscience \nApplication Statement Feedback Program \n\nDisclaimer: I volunteer as an editor for ASFP."
  },
  {
    "objectID": "resources.html#professional-development",
    "href": "resources.html#professional-development",
    "title": "Resources",
    "section": "Professional development",
    "text": "Professional development\n\nCreating a simple and effective academic personal website \nAmerican Psychological Association - Tips for Determining Authorship Credit"
  }
]